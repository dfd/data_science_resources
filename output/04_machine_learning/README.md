#Machine Learning  
## Local Table of Contents  
[(Back to Master Table of Contents)](../)  
[Machine Learning Textbooks](#MachineLearningTextbooks)  
[Machine Learning Tutorials](#MachineLearningTutorials)  
[Machine Learning Courses](#MachineLearningCourses)  
[Machine Learning Lectures](#MachineLearningLectures)  
[Machine Learning Blogs](#MachineLearningBlogs)  
[Machine Learning Podcasts](#MachineLearningPodcasts)  
[Machine Learning Packages](#MachineLearningPackages)  
[Machine Learning Misc](#MachineLearningMisc)  
## <a name="MachineLearningTextbooks"></a>Machine Learning Textbooks  

[Information Theory, Inference, and Learning Algorithms](http://www.inference.phy.cam.ac.uk/itila/)  
by David J.C. MacKay  
"This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks."  
Other tags: [Statistics Textbooks](../03_statistics#StatisticsTextbooks)   
  
[Mining Massive Datasets](http://www.mmds.org/#ver21)  
by Jure Leskovec, Anand Rajaraman, Jeff Ullman (Stanford University)  
"The book is based on Stanford Computer Science course CS246: Mining Massive Datasets (and CS345A: Data Mining). The book, like the course, is designed at the undergraduate computer science level with no formal prerequisites. To support deeper explorations, most of the chapters are supplemented with further reading references."  
  
[Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/)  
by Carl Edward Rasmussen and Christopher K. I. Williams  
"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes."  
  
[A First Encounter with Machine Learning](https://www.ics.uci.edu/~welling/teaching/ICS273Afall11/IntroMLBook.pdf)  
by Max Welling  
A nice introduction to various algorithms, with intuitive explanations of the formulas.  
Other tags: [Beginner Machine Learning](../01_beginner#BeginnerMachineLearning)   
  
[Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)  
by David Barber  
"The book is designed to appeal to students with only a modest mathematical background in undergraduate calculus and linear algebra. No formal computer science or statistical background is required to follow the book, although a basic familiarity with probability, calculus and linear algebra would be useful. The book should appeal to students from a variety of backgrounds, including Computer Science, Engineering, applied Statistics, Physics, and Bioinformatics that wish to gain an entry to probabilistic approaches in Machine Learning. In order to engage with students, the book introduces fundamental concepts in inference using only minimal reference to algebra and calculus. More mathematical techniques are postponed until as and when required, always with the concept as primary and the mathematics secondary."  
Other tags: [Statistics Textbooks](../03_statistics#StatisticsTextbooks)   
  
[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)  
by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani  
"This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist."  
Other tags: [Statistics Textbooks](../03_statistics#StatisticsTextbooks), [R Textbooks](../02_programming#RTextbooks), [Beginner Machine Learning](../01_beginner#BeginnerMachineLearning)   
  
[The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)  
by Trevor Hastie, Robert Tibshirani, Jerome Friedman  
"During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book."  
Other tags: [Statistics Textbooks](../03_statistics#StatisticsTextbooks), [R Textbooks](../02_programming#RTextbooks)   
  
[Foundations of Data Science](https://www.cs.cornell.edu/jeh/book2016June9.pdf)  
by Avrim Blum, John Hopcroft and Ravindran Kannan  
"This book starts with the treatment of high dimensional geometry... We focus on singular value decomposition, a central tool in this area... Central to our understanding of large structures, like the web and social networks, is building models to capture essential properties of these structures... We describe the foundations of machine learning, both algorithms for optimizing over given training examples, as well as the theory for understanding when such optimization can be expected to lead to good performance on new, unseen data... In Chapter 7 we study how to draw good samples efficiently and how to estimate statistical and linear algebra quantities, with such samples... After describing some of the basic methods for clustering, such as the k-means algorithm, we focus on modern developments in understanding these, as well as newer algorithms.  This book also covers graphical models and belief propagation, ranking and voting, sparse vectors, and compressed sensing. The appendix includes a wealth of background material.  
  
## <a name="MachineLearningTutorials"></a>Machine Learning Tutorials  

[Python Data Science Tutorials](https://github.com/ujjwalkarn/DataSciencePython)  
by Ujjwal Karn  
"This repo contains a curated list of Python tutorials for Data Science, NLP and Machine Learning."  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Machine Learning is Fun](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.jldbrt65w)  
by Adam Geitgey in Python  
Series of posts demonstrating various machine learning tasks using Python.  Examples include recurrent neural networks, convoluatoinal neural networks, face recognition, language translation, and speech recognition.  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Will it Python? Machine Learning for Hackers](https://github.com/carljv/Will_it_Python/tree/master/MLFH)  
by Carl Vogel  
Python implementations of examples from Machine Learning for Hackers. Uses Python 2.  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Using pandas and scikit-learn for classification tasks](https://github.com/jseabold/depy/blob/master/pandas_sklearn_rendered.ipynb)  
by Skipper Seabold  
Example of using Pandas and scikit-learn to classify whether a person makes over 50K a year.  
Other tags: [Python Pandas Tutorials](../02_programming#PythonPandasTutorials)   
  
[Using scikit-learn Pipelines and FeatureUnions](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)  
by Zac Stewart  
"The following is a moderately detailed explanation and a few examples of how I use pipelining when I work on competitions."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials)   
  
[Document Classification with scikit-learn](http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html)  
by Zac Stewart  
"To demonstrate text classification with scikit-learn, we're going to build a simple spam filter. While the filters in production for services like Gmail are vastly more sophisticated, the model we'll have by the end of this tutorial is effective, and surprisingly accurate."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials), [Spam Filter](../08_analytics#SpamFilter)   
  
[Guide to Model Stacking (i.e. Meta Ensembling)](https://gormanalysis.com/guide-to-model-stacking-i-e-meta-ensembling/)  
by Ben Gorman  
"Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model. Often times the stacked model (also called 2nd-level model) will outperform each of the individual models due its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. Here I provide a simple example and guide on how stacking is most often implemented in practice." Link to github repo with R code is provided.  
Other tags: [R Tutorials](../02_programming#RTutorials)   
  
[ROC Curves in Python and R](http://blog.yhat.com/posts/roc-curves.html)  
by yhat  
"Ever heard people at your office talking about AUC, ROC, or TPR but been too shy to ask what the heck they're talking about? Well lucky for you we're going to be diving into the wonderful world of binary classification evaluation today. In particular, we'll be discussing ROC curves."  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython), [R Tutorials](../02_programming#RTutorials)   
  
[Random forest interpretation with scikit-learn](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)  
by Ando Saabas  
Example of using package treeinterpreter for insight into a scikit-learn RandomForest  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Pattern Classification](https://github.com/rasbt/pattern_classification)  
by Sebastian Raschka  
A collection of tutorials and examples for solving and understanding machine learning and pattern classification tasks  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Out-of-core Learning and Model Persistence using scikit-learn](https://github.com/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb)  
by Sebastian Raschka  
"When we are applying machine learning algorithms to real-world applications, our computer hardware often still constitutes the major bottleneck of the learning process. Of course, we all have access to supercomputers, Amazon EC2, Apache Spark, etc. However, out-of-core learning via Stochastic Gradient Descent can still be attractive if we'd want to update our model on-the-fly ('online-learning'), and in this notebook, I want to provide some examples of how we can implement an 'out-of-core' approach using scikit-learn. I compiled the following code examples for personal reference, and I don't intend it to be a comprehensive reference for the underlying theory, but nonetheless, I decided to share it since it may be useful to one or the other!"  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials), [Sentiment Analysis](../08_analytics#SentimentAnalysis)   
  
[Python: scikit-learn – Training a classifier with non numeric features](https://www.webcodegeeks.com/python/python-scikit-learn-training-classifier-non-numeric-features/)  
by Mark Needham  
Tutorial on how to convert text data for training with a scikit-learn RandomForest Classifier  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials)   
  
[Implementing a Weighted Majority Rule Ensemble Classifier](http://sebastianraschka.com/Articles/2014_ensemble_classifier.html)  
by Sebastian Raschka  
"Here, I want to present a simple and conservative approach of implementing a weighted majority rule ensemble classifier in scikit-learn that yielded remarkably good results when I tried it in a kaggle competition."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials)   
  
[Data Science and (Unsupervised) Machine Learning with scikit-learn](http://opensource.datacratic.com/mtlpy50/)  
by Nicolas Kruchten  
"A different way to look at graph analysis and visualization, as an introduction to a few cool algorithms: Truncated SVD, K-Means and t-SNE with a practical walkthrough using scikit-learn and friends numpy and bokeh, and finishing off with some more general commentary on this approach to data analysis."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials), [Document Classification](../08_analytics#DocumentClassification)   
  
[Classifier calibration with Platt's scaling and isotonic regression](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/)  
by Zygmunt Zając  
"Calibration is applicable in case a classifier outputs probabilities. Apparently some classifiers have their typical quirks - for example, they say boosted trees and SVM tend to predict probabilities conservatively, meaning closer to mid-range than to extremes. If your metric cares about exact probabilities, like logarithmic loss does, you can calibrate the classifier, that is post-process the predictions to get better estimates."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials)   
  
[Edge Prediction in a Social Graph: My Solution to Facebook's User Recommendation Contest on Kaggle](http://blog.echen.me/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/)  
by Edwin Chen  
"A couple weeks ago, Facebook launched a link prediction contest on Kaggle, with the goal of recommending missing edges in a social graph. I love investigating social networks, so I dug around a little, and since I did well enough to score one of the coveted prizes, I’ll share my approach here."  
Other tags: [Social Networks](../08_analytics#SocialNetworks)   
  
[Nonparametric Bayesian Regression with Gaussian Processes](http://austinrochford.com/posts/2014-03-23-bayesian-nonparamtric-regression-gp.html)  
by Austin Rochford  
"In this post, we’ll explore a Bayesian approach to nonparametric regression, which allows us to model complex functions with relatively weak assumptions." Extends scikit-learn.  
Other tags: [Tutorials in Python](../02_programming#TutorialsinPython)   
  
[Bayesian Optimization of Machine Learning Models](http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html)  
by Max Kuhn  
"I'll demonstrate how Bayesian optimization and Gaussian process models can be used as an alternative."  
Other tags: [R Tutorials](../02_programming#RTutorials)   
  
[A demonstration of vtreat data preparation](http://www.win-vector.com/blog/2016/06/a-demonstration-of-vtreat-data-preparation/)  
by John Mount  
"This article is a demonstration the use of the R vtreat variable preparation package followed by caret controlled training."  
Other tags: [R Tutorials](../02_programming#RTutorials)   
  
[Feature Selection with caret's Genetic Algorithm Option](http://blog.revolutionanalytics.com/2015/12/caret-genetic.html)  
by Joseph Rickert  
"Performing feature selection with GAs requires conceptualizing the process of feature selection as an optimization problem and then mapping it to the genetic framework of random variation and natural selection."  
Other tags: [R Tutorials](../02_programming#RTutorials)   
  
[Confidence Intervals for Random Forests](http://blog.revolutionanalytics.com/2016/03/confidence-intervals-for-random-forest.html)  
by Joseph Rickert  
"Here, I fit a randomForest model to eight features from the UCI MPG data set and use the randomForestInfJack() function to calculate the infinitesimal Jackknife estimator."  
Other tags: [R Tutorials](../02_programming#RTutorials)   
  
[PyCon 2015 Introduction to Scikit-Learn tutorial](http://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/Index.ipynb)  
by Jake Vanderplas  
"Statistics has the reputation of being difficult to understand, but using some simple Python skills it can be made much more intuitive. This talk will cover several sampling-based approaches to solving statistical problems, and show you that if you can write a for-loop, you can do statistics."  
Other tags: [Python scikit-learn Tutorials](../02_programming#Pythonscikit-learnTutorials)   
  
## <a name="MachineLearningCourses"></a>Machine Learning Courses  

[Machine Learning](https://www.coursera.org/learn/machine-learning)  
by Andrew Ng, Stanford University  
"This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas."  
Other tags: [Beginner Machine Learning](../01_beginner#BeginnerMachineLearning)   
  
[Practical Deep Learning for Coders, Part 1](http://course.fast.ai/)  
by Jeremy Howard  
"Learn how to build state of the art models without needing graduate-level math—but also without dumbing anything down."  
  
[Learning From Data (Introductory Machine Learning)](https://www.edx.org/course/learning-data-introductory-machine-caltechx-cs1156x)  
by Yaser S. Abu-Mostafa (Caltech)  
"Introductory Machine Learning course covering theory, algorithms and applications. Our focus is on real understanding, not just 'knowing.'"  
Other tags: [Beginner Machine Learning](../01_beginner#BeginnerMachineLearning)   
  
[Probabilistic Graphical Models](https://www.coursera.org/learn/probabilistic-graphical-models)  
by Daphne Koller, Stanford University  
"In this class, you will learn the basics of the PGM representation and how to construct them, using both human knowledge and machine learning techniques; you will also learn algorithms for using a PGM to reach conclusions about the world from limited and noisy evidence, and for making good decisions under uncertainty. The class covers both the theoretical underpinnings of the PGM framework and practical skills needed to apply these techniques to new problems."  Sign up for a previous offering of the course.  
  
[Programming with Python for Data Science](https://www.edx.org/course/programming-python-data-science-microsoft-dat210x-2)  
by Authman Apatira, Microsoft  
"In this practical computer science course, you will build on your existing Python skills and learn how to manipulate data using Pandas, and build machine learning solutions in Python using the scikit-learn package."  
Other tags: [Python Courses](../02_programming#PythonCourses)   
  
[Programming with R for Data Science](https://www.edx.org/course/programming-r-data-science-microsoft-dat209x-2)  
by Anders Stockmarr  
"In this course you will learn all you need to get up to speed with programming in R. Explore R data structures and syntaxes, see how to read and write data from a local file to a cloud-hosted database, work with data, get summaries, and transform them to fit your needs. Plus, find out how to perform predictive analytics using R and how to create visualizations using the popular ggplot2 package."  
Other tags: [R Courses](../02_programming#RCourses)   
  
[Distributed Machine Learning with Apache Spark](https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x)  
by Ameet Talwalkar, UCLA and Jon Bates, Databricks  
"This statistics and data analysis course introduces the underlying statistical and algorithmic principles required to develop scalable real-world machine learning pipelines. We present an integrated view of data processing by highlighting the various components of these pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. You will gain hands-on experience applying these principles using Spark, a cluster computing system well-suited for large-scale machine learning tasks, and its packages spark.ml and spark.mllib. You will implement distributed algorithms for fundamental statistical models (linear regression, logistic regression, principal component analysis) while tackling key problems from domains such as online advertising and cognitive neuroscience."  
Other tags: [Spark Courses](../02_programming#SparkCourses)   
  
[Advanced Distributed Machine Learning with Apache Spark](https://www.edx.org/course/advanced-distributed-machine-learning-uc-berkeleyx-cs125x)  
by Ameet Talwalkar, UCLA and Jon Bates, Databricks  
"Building on the core ideas presented in Distributed Machine Learning with Spark, this course covers advanced topics for training and deploying large-scale learning pipelines. You will study state-of-the-art distributed algorithms for collaborative filtering, ensemble methods (e.g., random forests), clustering and topic modeling, with a focus on model parallelism and the crucial tradeoffs between computation and communication."  
Other tags: [Spark Courses](../02_programming#SparkCourses)   
  
[Statistical Learning](http://online.stanford.edu/course/statistical-learning-Winter-16)  
by Trevor Hastie and Rob Tibshirani, Stanford University  
"This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical)."  
Other tags: [Statistics Courses](../03_statistics#StatisticsCourses), [R Courses](../02_programming#RCourses)   
  
## <a name="MachineLearningLectures"></a>Machine Learning Lectures  

[Machine Learning Playlist](https://www.youtube.com/user/mathematicalmonk/playlists)  
by mathematicalmonk  
160 Machine Learning Short Lectures  
  
[Learning From Data](http://work.caltech.edu/lectures.html)  
by Yaser Abu-Mostafa (Caltech)  
Machine Learning course from Caltech. "The fundamental concepts and techniques are explained in detail. The focus of the lectures is real understanding, not just 'knowing.'"  
  
[Data Science](http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml)  
by Joe Blitzstein, Hanspeter Pfister, Verena Kaynig-Fittkau (Harvard University)  
Lectures from Harvard Extension School's Data Science class  
Other tags: [Python Lectures](../02_programming#PythonLectures)   
  
[R: Applied Predictive Modeling](https://www.youtube.com/watch?v=99lnTku75Pc)  
by Max Kuhn  
"Max Kuhn, author of Applied Predictive Modeling and caret package, will talk about the practice of predictive modeling. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model's prediction accuracy on future data."  
  
## <a name="MachineLearningBlogs"></a>Machine Learning Blogs  

[Win-Vector](http://www.win-vector.com/blog/)  
by John Mount and Nina Zumel  
The Win-Vector LLC data science blog  
Other tags: [Statistics Blogs](../03_statistics#StatisticsBlogs), [Programming Blogs](../02_programming#ProgrammingBlogs)   
  
[No Free Hunch](http://blog.kaggle.com/)  
by Kaggle  
Interviews of Kaggle Compeition top finishers, and scripts shared by competitors.  
  
[Sebatian Raschka's Blog](http://sebastianraschka.com/blog/index.html)  
by Sebatian Raschka  
Blog of the author of the 'Python Machine Learning' book  
  
[Fast ML](http://fastml.com/)  
by Zygmunt Zając  
"This site is brought to you by the letters “M” and “L”. It is meant to tackle interesting topics in machine learning while being entertaining and easy to read and understand. FastML probably grew out of a frustration with papers you need a PhD in math to understand and with either no code or half-baked Matlab implementation of homework-assignment quality. We understand that some cutting-edge researchers might have no interest in providing the goodies for free, or just no interest in such down-to-earth matters. But we don't have time nor desire to become experts in every machine learning topic. Fortunately, there is quite a lot of good software with acceptable documentation."  
  
## <a name="MachineLearningPodcasts"></a>Machine Learning Podcasts  

[Talking Machines](http://www.thetalkingmachines.com/)  
by Katherine Gorman and Ryan Adams (Harvard University)  
"Talking Machines is your window into the world of machine learning. Your hosts, Katherine Gorman and Ryan Adams, bring you clear conversations with experts in the field, insightful discussions of industry news, and useful answers to your questions."  
  
[Partially Derivative](http://partiallyderivative.com/)  
by Jonathon Morgan, Vidya Spandana, and Chris Albon  
"Partially Derivative is a weekly podcast, blog, and newsletter about the data science in the world around us. We geek out on silly data stories, interview amazing people doing interesting work, and drink beer."  
  
[Learning Machines 101](http://www.learningmachines101.com/)  
by Richard M. Golden  
The intended audience for this podcast series is the general public and the intended objective of this podcast series is to help popularize and demystify the field of Artificial Intelligence by explaining fundamental concepts in an entertaining manner. However, many advanced topics in artificial intelligence and machine learning will be discussed at a “high-level” so students, scientists, and engineers working in the machine learning area may find this podcast series beneficial for identifying relevant “entry points” into advanced statistical machine learning topics. Relevant references to advanced readings are provided (when applicable) in the show notes for each episode.  
Other tags: [Deep Learning Podcasts](../05_deep_learning#DeepLearningPodcasts), [Reinforcement Learning Podcasts](../06_reinforement_learning#ReinforcementLearningPodcasts)   
  
[O'Reilly Data Show Podcast](https://www.oreilly.com/topics/oreilly-data-show-podcast)  
by Ben Lorica (O'Reilly)  
"The O'Reilly Data Show Podcast: Danny Bickson on recommenders, data science, and applications of machine learning."  
  
## <a name="MachineLearningPackages"></a>Machine Learning Packages  

[scikit-learn](http://scikit-learn.org/stable/)  
"Simple and efficient tools for data mining and data analysis."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[NLTK](http://www.nltk.org/)  
"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[spaCy](https://spacy.io/)  
"spaCy helps you write programs that do clever things with text. You give it a string of characters, it gives you an object that provides multiple useful views of its meaning and linguistic structure. Specifically, spaCy features a high performance tokenizer, part-of-speech tagger, named entity recognizer and syntactic dependency parser, with built-in support for word vectors. All of the functionality is united behind a clean high-level Python API, that makes it easy to use the different annotations together."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[gensim](http://radimrehurek.com/gensim/index.html)  
"topic modelling for humans"  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)  
"Fuzzy string matching like a boss. It uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[OpenCV-Python](http://opencv.org/)  
"OpenCV is released under a BSD license and hence it’s free for both academic and commercial use. It has C++, C, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[Simple CV](http://simplecv.org/)  
"SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries such as OpenCV – without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage. This is computer vision made easy."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[scikit-image](http://scikit-image.org/)  
"scikit-image is a collection of algorithms for image processing. It is available free of charge and free of restriction. We pride ourselves on high-quality, peer-reviewed code, written by an active community of volunteers."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[TFLearn](http://tflearn.org/)  
"TFlearn is a modular and transparent deep learning library built on top of Tensorflow. It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[XGBoost (Python)](http://xgboost.readthedocs.io/en/latest/python/python_intro.html)  
An optimized, flexible, portable, regression and classification gradient boosting package that supports distributed training.  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[tsfresh](http://tsfresh.readthedocs.io/en/latest/)  
"tsfresh is a python package that is used to automatically calculate a huge number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks."  
Other tags: [Python Packages](../02_programming#PythonPackages)   
  
[XGBoost (R)](http://xgboost.readthedocs.io/en/latest/R-package/index.html)  
An optimized, flexible, portable, regression and classification gradient boosting package that supports distributed training.  
Other tags: [R Packages](../02_programming#RPackages)   
  
[caret](http://topepo.github.io/caret/index.html)  
"The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:  data splitting; pre-processing; feature selection; model tuning using resampling; variable importance estimation."  
Other tags: [R Packages](../02_programming#RPackages)   
  
[Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)  
by Joseph Misiti  
A curated list of awesome machine learning frameworks, libraries and software (by language).   
Other tags: [Python Packages](../02_programming#PythonPackages), [R Packages](../02_programming#RPackages)   
  
## <a name="MachineLearningMisc"></a>Machine Learning Misc  

[scikit-learn algorithm cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/)  
Flow chart for Machine Learning Algorithms in scikit-learn.  
Other tags: [Beginner Machine Learning](../01_beginner#BeginnerMachineLearning)   
  
[Model evaluation, model selection, and algorithm selection in machine learning](http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)  
by Sebatian Raschka  
Part 1: <a href='http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html'>General ideas behind model evaluation in supervised machine learning</a><br>Part 2: <a href='http://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html'>Some advanced techniques for model evaluation</a><br>Part 3: <a href='https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html'>Model evaluation, model selection, and algorithm selection in machine learning</a>  
  
[Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/)  
by Kaggle user Triskelion  
"Model ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions."  
  
[An Interactive Tutorial on Numerical Optimization](http://www.benfrederickson.com/numerical-optimization/)  
by Ben Frederickson  
"Numerical Optimization is one of the central techniques in Machine Learning. For many problems it is hard to figure out the best solution directly, but it is relatively easy to set up a loss function that measures how good a solution is - and then minimize the parameters of that function to find the solution."  
  
[Predicting with confidence: the best machine learning idea you never heard of](https://scottlocklin.wordpress.com/2016/12/05/predicting-with-confidence-the-best-machine-learning-idea-you-never-heard-of/)  
by Scott Locklin  
Explanation of conformal prediction for constructing confidence intervals for machine learning algorithms  
  
[How to Use t-SNE Effectively](http://distill.pub/2016/misread-tsne/)  
by Martin Wattenberg, Fernanda Viégas, Ian Johnson  
"Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. By exploring how it behaves in simple cases, we can learn to use it more effectively."  
Other tags: [Visualization Misc](../07_visualizaiton#VisualizationMisc)   
  
[Predictive Model Deployment with Spark](http://commitlogs.com/2016/11/19/predictive-model-deployment-with-spark/)  
by Lei Gong  
"However, deploying predictive model to a production environment, or serving the model in production, is a bit more complicated. Its architecture largely depends on how the model will be used. At very high level, predictive models often are used to score some instances, e.g. the risk score of fraud transaction or the likelihood of clicking on ads. This scoring operation can be offline or online, depending on its application. Offline scoring means the model doesn’t needs to score an instance in real-time and online scoring means the model is required to score with real-time input and low-latency. In this post, I am going to touch on a few common architectures and their use cases.<p>Follow up post <a href='http://commitlogs.com/2016/12/17/predictive-model-deployment-missing-components/'>about continuous deployment aspects here</a>."  
Other tags: [Programming Misc](../02_programming#ProgrammingMisc)   
  
