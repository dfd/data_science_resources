#Machine Learning  
## Local Table of Contents  
[(Back to Master Table of Contents)](../)  
[Machine Learning Textbooks](#MachineLearningTextbooks)  
[Machine Learning Tutorials](#MachineLearningTutorials)  
[Machine Learning Courses](#MachineLearningCourses)  
[Machine Learning Lectures](#MachineLearningLectures)  
[Machine Learning Blogs](#MachineLearningBlogs)  
[Machine Learning Podcasts](#MachineLearningPodcasts)  
[Machine Learning Packages](#MachineLearningPackages)  
[Machine Learning Misc](#MachineLearningMisc)  
## <a name="MachineLearningTextbooks"></a>Machine Learning Textbooks  

[Information Theory, Inference, and Learning Algorithms](http://www.inference.phy.cam.ac.uk/itila/)  
by David J.C. MacKay  
"This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or secondyear undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks."  
Other tags: [Statistics Textbooks](../statistics#StatisticsTextbooks)   
  
[Mining Massive Datasets](http://www.mmds.org/#ver21)  
by Jure Leskovec, Anand Rajaraman, Jeff Ullman (Stanford University)  
"The book is based on Stanford Computer Science course CS246: Mining Massive Datasets (and CS345A: Data Mining). The book, like the course, is designed at the undergraduate computer science level with no formal prerequisites. To support deeper explorations, most of the chapters are supplemented with further reading references."  
  
[Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/)  
by Carl Edward Rasmussen and Christopher K. I. Williams  
"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes."  
  
[A First Encounter with Machine Learning](https://www.ics.uci.edu/~welling/teaching/ICS273Afall11/IntroMLBook.pdf)  
by Max Welling  
A nice introduction to various algorithms, with intuitive explanations of the formulas.  
  
[Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online)  
by David Barber  
"The book is designed to appeal to students with only a modest mathematical background in undergraduate calculus and linear algebra. No formal computer science or statistical background is required to follow the book, although a basic familiarity with probability, calculus and linear algebra would be useful. The book should appeal to students from a variety of backgrounds, including Computer Science, Engineering, applied Statistics, Physics, and Bioinformatics that wish to gain an entry to probabilistic approaches in Machine Learning. In order to engage with students, the book introduces fundamental concepts in inference using only minimal reference to algebra and calculus. More mathematical techniques are postponed until as and when required, always with the concept as primary and the mathematics secondary."  
Other tags: [Statistics Textbooks](../statistics#StatisticsTextbooks)   
  
[Reinforcement Learning: An Introduction ](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html)  
by Richard S. Sutton and Andrew G. Barto  
"The book consists of three parts. Part I is introductory and problem oriented. We focus on the simplest aspects of reinforcement learning and on its main distinguishing features. One full chapter is devoted to introducing the reinforcement learning problem whose solution we explore in the rest of the book. Part II presents what we see as the three most important elementary solution methods: dynamic programming, simple Monte Carlo methods, and temporal-difference learning. The first of these is a planning method and assumes explicit knowledge of all aspects of a problem, whereas the other two are learning methods. Part III is concerned with generalizing these methods and blending them. Eligibility traces allow unification of Monte Carlo and temporal-difference methods, and function approximation methods such as artificial neural networks extend all the methods so that they can be applied to much larger problems. We bring planning and learning methods together again and relate them to heuristic search. Finally, we summarize our view of the state of reinforcement learning research and briefly present case studies, including some of the most impressive applications of reinforcement learning to date."  
  
[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)  
by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani  
"This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist."  
Other tags: [Statistics Textbooks](../statistics#StatisticsTextbooks), [R Textbooks](../programming#RTextbooks)   
  
[The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)  
by Trevor Hastie, Robert Tibshirani, Jerome Friedman  
"During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book."  
Other tags: [Statistics Textbooks](../statistics#StatisticsTextbooks), [R Textbooks](../programming#RTextbooks)   
  
[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)  
by Michael Nielsen  
"The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data; Deep learning, a powerful set of techniques for learning in neural networks."  Uses Python 2.  
  
[Deep Learning](http://www.deeplearningbook.org/)  
by Ian Goodfellow, Yoshua Bengio and Aaron Courville  
"The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular." A pdf version appears to be available here: https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf  
  
## <a name="MachineLearningTutorials"></a>Machine Learning Tutorials  

[Will it Python? Machine Learning for Hackers](https://github.com/carljv/Will_it_Python/tree/master/MLFH)  
by Carl Vogel  
Python implementations of examples from Machine Learning for Hackers. Uses Python 2.  
Other tags: [Python Tutorials](../programming#PythonTutorials)   
  
[Diving into Machine Learning through TensorFlow - PyCon 2016](https://www.youtube.com/watch?v=GZBIPwdGtkk)  
by Julia Ferraioli, Amy Unruh, Eli Bixby  
"Machine learning can be an intimidating subject. In this session, we’ll get practical, hands-on experience with core concepts in machine learning with TensorFlow, an open source deep learning library. We’ll introduce the basics of TensorFlow, including how to ingest and prepare raw data for use, run a variety of algorithms to gain insight from the data, and have some fun with visualization."  
Other tags: [Python Tutorials](../programming#PythonTutorials)   
  
## <a name="MachineLearningCourses"></a>Machine Learning Courses  

[Machine Learning](https://www.coursera.org/learn/machine-learning)  
by Andrew Ng, Stanford University  
"This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas."  
  
[Probabilistic Graphical Models](https://class.coursera.org/pgm/lecture/preview)  
by Daphne Koller, Stanford University  
"In this class, you will learn the basics of the PGM representation and how to construct them, using both human knowledge and machine learning techniques; you will also learn algorithms for using a PGM to reach conclusions about the world from limited and noisy evidence, and for making good decisions under uncertainty. The class covers both the theoretical underpinnings of the PGM framework and practical skills needed to apply these techniques to new problems."  Sign up for a previous offering of the course.  
  
[Programming with Python for Data Science](https://www.edx.org/course/programming-python-data-science-microsoft-dat210x)  
by Authman Apatira, Microsoft  
"In this practical computer science course, you will build on your existing Python skills and learn how to manipulate data using Pandas, and build machine learning solutions in Python using the scikit-learn package."  
Other tags: [Python Courses](../programming#PythonCourses)   
  
[Programming with R for Data Science](https://www.edx.org/course/programming-r-data-science-microsoft-dat209x-0)  
by Anders Stockmarr  
"In this course you will learn all you need to get up to speed with programming in R. Explore R data structures and syntaxes, see how to read and write data from a local file to a cloud-hosted database, work with data, get summaries, and transform them to fit your needs. Plus, find out how to perform predictive analytics using R and how to create visualizations using the popular ggplot2 package."  
Other tags: [R Courses](../programming#RCourses)   
  
[Distributed Machine Learning with Apache Spark](https://www.edx.org/course/distributed-machine-learning-apache-uc-berkeleyx-cs120x)  
by Ameet Talwalkar, UCLA and Jon Bates, Databricks  
"This statistics and data analysis course introduces the underlying statistical and algorithmic principles required to develop scalable real-world machine learning pipelines. We present an integrated view of data processing by highlighting the various components of these pipelines, including exploratory data analysis, feature extraction, supervised learning, and model evaluation. You will gain hands-on experience applying these principles using Spark, a cluster computing system well-suited for large-scale machine learning tasks, and its packages spark.ml and spark.mllib. You will implement distributed algorithms for fundamental statistical models (linear regression, logistic regression, principal component analysis) while tackling key problems from domains such as online advertising and cognitive neuroscience."  
Other tags: [Spark Courses](../programming#SparkCourses)   
  
[Advanced Distributed Machine Learning with Apache Spark](https://www.edx.org/course/advanced-distributed-machine-learning-uc-berkeleyx-cs125x)  
by Ameet Talwalkar, UCLA and Jon Bates, Databricks  
"Building on the core ideas presented in Distributed Machine Learning with Spark, this course covers advanced topics for training and deploying large-scale learning pipelines. You will study state-of-the-art distributed algorithms for collaborative filtering, ensemble methods (e.g., random forests), clustering and topic modeling, with a focus on model parallelism and the crucial tradeoffs between computation and communication."  
Other tags: [Spark Courses](../programming#SparkCourses)   
  
[Statistical Learning](http://online.stanford.edu/course/statistical-learning-Winter-16)  
by Trevor Hastie and Rob Tibshirani, Stanford University  
"This is an introductory-level course in supervised learning, with a focus on regression and classification methods. The syllabus includes: linear and polynomial regression, logistic regression and linear discriminant analysis; cross-validation and the bootstrap, model selection and regularization methods (ridge and lasso); nonlinear models, splines and generalized additive models; tree-based methods, random forests and boosting; support-vector machines. Some unsupervised learning methods are discussed: principal components and clustering (k-means and hierarchical)."  
Other tags: [Statistics Courses](../statistics#StatisticsCourses), [R Courses](../programming#RCourses)   
  
## <a name="MachineLearningLectures"></a>Machine Learning Lectures  

[Machine Learning Playlist](https://www.youtube.com/user/mathematicalmonk/playlists)  
by mathematicalmonk  
160 Machine Learning Short Lectures  
  
[Learning From Data](http://work.caltech.edu/lectures.html)  
by Yaser Abu-Mostafa (Caltech)  
Machine Learning course from Caltech. "The fundamental concepts and techniques are explained in detail. The focus of the lectures is real understanding, not just 'knowing.'"  
  
[Data Science](http://cm.dce.harvard.edu/2014/01/14328/publicationListing.shtml)  
by Joe Blitzstein, Hanspeter Pfister, Verena Kaynig-Fittkau (Harvard University)  
  
Other tags: [Python Lectures](../programming#PythonLectures)   
  
## <a name="MachineLearningBlogs"></a>Machine Learning Blogs  

[Win-Vector](http://www.win-vector.com/blog/)  
by John Mount and Nina Zumel  
The Win-Vector LLC data science blog  
Other tags: [Statistics Blogs](../statistics#StatisticsBlogs), [Programming Blogs](../programming#ProgrammingBlogs)   
  
[No Free Hunch](http://blog.kaggle.com/)  
by Kaggle  
Interviews of Kaggle Compeition top finishers, and scripts shared by competitors.  
  
[Sebatian Raschka's Blog](http://sebastianraschka.com/blog/index.html)  
by Sebatian Raschka  
Blog of the author of the 'Python Machine Learning' book  
  
[Fast ML](http://fastml.com/)  
by Zygmunt Zając  
"This site is brought to you by the letters “M” and “L”. It is meant to tackle interesting topics in machine learning while being entertaining and easy to read and understand. FastML probably grew out of a frustration with papers you need a PhD in math to understand and with either no code or half-baked Matlab implementation of homework-assignment quality. We understand that some cutting-edge researchers might have no interest in providing the goodies for free, or just no interest in such down-to-earth matters. But we don't have time nor desire to become experts in every machine learning topic. Fortunately, there is quite a lot of good software with acceptable documentation."  
  
## <a name="MachineLearningPodcasts"></a>Machine Learning Podcasts  

[Talking Machines](http://www.thetalkingmachines.com/)  
by Katherine Gorman and Ryan Adams (Harvard University)  
"Talking Machines is your window into the world of machine learning. Your hosts, Katherine Gorman and Ryan Adams, bring you clear conversations with experts in the field, insightful discussions of industry news, and useful answers to your questions."  
  
[Partially Derivative](http://partiallyderivative.com/)  
by Jonathon Morgan, Vidya Spandana, and Chris Albon  
"Partially Derivative is a weekly podcast, blog, and newsletter about the data science in the world around us. We geek out on silly data stories, interview amazing people doing interesting work, and drink beer."  
  
[O'Reilly Data Show Podcast](https://www.oreilly.com/topics/oreilly-data-show-podcast)  
by Ben Lorica (O'Reilly)  
"The O'Reilly Data Show Podcast: Danny Bickson on recommenders, data science, and applications of machine learning."  
  
## <a name="MachineLearningPackages"></a>Machine Learning Packages  

[scikit-learn](http://scikit-learn.org/stable/)  
"Simple and efficient tools for data mining and data analysis."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[TensorFlow](https://www.tensorflow.org/)  
"TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization. The system is designed to facilitate research in machine learning, and to make it quick and easy to transition from research prototype to production system."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[NLTK](http://www.nltk.org/)  
"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[spaCy](https://spacy.io/)  
"spaCy helps you write programs that do clever things with text. You give it a string of characters, it gives you an object that provides multiple useful views of its meaning and linguistic structure. Specifically, spaCy features a high performance tokenizer, part-of-speech tagger, named entity recognizer and syntactic dependency parser, with built-in support for word vectors. All of the functionality is united behind a clean high-level Python API, that makes it easy to use the different annotations together."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)  
"Fuzzy string matching like a boss. It uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[OpenCV-Python](http://opencv.org/)  
"OpenCV is released under a BSD license and hence it’s free for both academic and commercial use. It has C++, C, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[Simple CV](http://simplecv.org/)  
"SimpleCV is an open source framework for building computer vision applications. With it, you get access to several high-powered computer vision libraries such as OpenCV – without having to first learn about bit depths, file formats, color spaces, buffer management, eigenvalues, or matrix versus bitmap storage. This is computer vision made easy."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[scikit-image](http://scikit-image.org/)  
"scikit-image is a collection of algorithms for image processing. It is available free of charge and free of restriction. We pride ourselves on high-quality, peer-reviewed code, written by an active community of volunteers."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[Theano](http://deeplearning.net/software/theano/)  
"Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[Keras](http://keras.io/)  
"Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research."  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[XGBoost (Python)](http://xgboost.readthedocs.io/en/latest/python/python_intro.html)  
An optimized, flexible, portable, regression and classification gradient boosting package that supports distributed training.  
Other tags: [Python Packages](../programming#PythonPackages)   
  
[XGBoost (R)](http://xgboost.readthedocs.io/en/latest/R-package/index.html)  
An optimized, flexible, portable, regression and classification gradient boosting package that supports distributed training.  
Other tags: [R Packages](../programming#RPackages)   
  
[caret](http://topepo.github.io/caret/index.html)  
"The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:  data splitting; pre-processing; feature selection; model tuning using resampling; variable importance estimation."  
Other tags: [R Packages](../programming#RPackages)   
  
## <a name="MachineLearningMisc"></a>Machine Learning Misc  

[scikit-learn algorithm cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/)  
Flow chart for Machine Learning Algorithms in scikit-learn.  
  
[Model evaluation, model selection, and algorithm selection in machine learning - Part I](http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)  
by Sebatian Raschka  
  
  
